# ðŸ§  Intelligent Crawl4AI Agent - Environment Configuration
# Copy this file to .env and configure your settings

# ====================================
# ðŸ’° COST-OPTIMIZED AI PROVIDERS (RECOMMENDED ORDER)
# ====================================

# 1st Priority: DeepSeek (Best value - $0.14/1M tokens)
DEEPSEEK_API_KEY=your-deepseek-api-key-here
DEEPSEEK_MODEL=deepseek-chat

# 2nd Priority: Groq (Fast and cheap)
GROQ_API_KEY=your-groq-api-key-here
GROQ_MODEL=llama-3.1-8b-instant

# 3rd Priority: Local Ollama (Free)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# 4th Priority: OpenRouter (Free models available)
OPENROUTER_API_KEY=your-openrouter-api-key-here
OPENROUTER_MODEL=meta-llama/llama-3.1-8b-instruct:free

# 5th Priority: OpenAI (Premium - more expensive)
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o-mini

# 6th Priority: Anthropic (Most expensive)
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-3-haiku-20240307

# ====================================
# ðŸŽ¯ AI OPTIMIZATION STRATEGY
# ====================================
# Choose: "cost", "speed", or "quality"
AI_OPTIMIZATION_STRATEGY=cost
CONFIDENCE_THRESHOLD=0.7
MAX_AI_RETRIES=3
ENABLE_AI_CACHING=true

# AI Enhancement Controls (to reduce API calls)
ENABLE_PATTERN_MATCHING=true
ENABLE_RULE_BASED_SELECTION=true
AI_ENHANCEMENT_THRESHOLD=0.8  # Only use AI if confidence < 0.8

# ====================================
# ðŸ“Š TRAINING DATA COLLECTION
# ====================================
ENABLE_TRAINING_DATA_COLLECTION=true
TRAINING_DATA_PATH=./data/training
LOG_AI_INTERACTIONS=true
STORE_SUCCESSFUL_PATTERNS=true

# ====================================
# âš¡ PERFORMANCE OPTIMIZATION
# ====================================
# Caching (reduces AI calls)
ANALYSIS_CACHE_TTL=86400      # 24 hours
STRATEGY_CACHE_TTL=3600       # 1 hour  
PATTERN_CACHE_TTL=604800      # 1 week

# Processing
MAX_WORKERS=50
MAX_CONCURRENT_PER_WORKER=10
RATE_LIMIT_PER_MINUTE=1000

# ====================================
# ðŸ—„ï¸ DATABASE CONFIGURATION
# ====================================
DATABASE_TYPE=postgresql
POSTGRES_URL=postgresql://scraper_user:secure_password_123@localhost:5432/intelligent_scraping
REDIS_URL=redis://localhost:6379
CHROMADB_URL=http://localhost:8000

# ====================================
# ðŸŒ WEB SERVER
# ====================================
WEB_HOST=0.0.0.0
WEB_PORT=8080
WEB_WORKERS=4

# ====================================
# ðŸŽ›ï¸ FEATURE TOGGLES
# ====================================
ENABLE_CHAT_INTERFACE=true
ENABLE_API_ENDPOINTS=true
ENABLE_WEBSOCKET=true
ENABLE_BACKGROUND_JOBS=true
ENABLE_REAL_TIME_STREAMING=true
ENABLE_PATTERN_LEARNING=true
ENABLE_AUTONOMOUS_TOOLS=true
ENABLE_PROGRESS_TRACKING=true

# AI Features (can be disabled to reduce API calls)
ENABLE_PRE_ANALYSIS=true
ENABLE_QUALITY_VALIDATION=true
ENABLE_DATA_ENRICHMENT=false    # Disable to save AI calls
ENABLE_ANALYTICS_TRACKING=false # Disable to save AI calls

# ====================================
# ðŸ“Š MONITORING & LOGGING
# ====================================
LOG_LEVEL=INFO
PYTHONUNBUFFERED=1
ENABLE_METRICS=true
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000

# AI Usage Tracking
LOG_AI_PROVIDER_USAGE=true
LOG_AI_COSTS=true
LOG_AI_PERFORMANCE=true

# ====================================
# ðŸ” SECURITY
# ====================================
SESSION_SECRET_KEY=your-super-secret-key-here
# API_KEY_REQUIRED=false
# CORS_ORIGINS=["http://localhost:8888"]

# ====================================
# ðŸ› ï¸ DEVELOPMENT
# ====================================
ENVIRONMENT=production
DEBUG=false
AUTO_RELOAD=false

# ====================================
# ðŸ’¡ USAGE EXAMPLES
# ====================================
# For MINIMAL AI usage (1-2 calls per request):
# ENABLE_PRE_ANALYSIS=false
# ENABLE_DATA_ENRICHMENT=false
# ENABLE_QUALITY_VALIDATION=false
# AI_ENHANCEMENT_THRESHOLD=0.9

# For MAXIMUM AI usage (5-8 calls per request):
# ENABLE_PRE_ANALYSIS=true
# ENABLE_DATA_ENRICHMENT=true
# ENABLE_QUALITY_VALIDATION=true
# AI_ENHANCEMENT_THRESHOLD=0.5

# For COST OPTIMIZATION:
# Use DeepSeek first: Set DEEPSEEK_API_KEY
# Enable caching: ENABLE_AI_CACHING=true
# Use patterns: ENABLE_PATTERN_MATCHING=true