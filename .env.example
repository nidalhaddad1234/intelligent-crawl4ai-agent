# Intelligent Crawl4AI Agent - Environment Configuration
# Copy this file to .env and customize the values for your setup

# =====================================================
# MCP SERVER CONFIGURATION
# =====================================================

# MCP Server Settings
MCP_SERVER_HOST=0.0.0.0
MCP_SERVER_PORT=8811
MCP_TIMEOUT=300

# =====================================================
# AI SERVICES CONFIGURATION
# =====================================================

# Ollama Local AI Service
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL_MAIN=llama3.1
OLLAMA_MODEL_EMBEDDING=nomic-embed-text

# ChromaDB Vector Database
CHROMADB_URL=http://localhost:8000
CHROMADB_HOST=localhost
CHROMADB_PORT=8000

# =====================================================
# HIGH-VOLUME PROCESSING INFRASTRUCTURE
# =====================================================

# Redis for Job Queuing and Session Management
REDIS_URL=redis://localhost:6379
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# PostgreSQL for Persistent Data Storage
POSTGRES_URL=postgresql://scraper_user:secure_password_123@localhost:5432/intelligent_scraping
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=intelligent_scraping
POSTGRES_USER=scraper_user
POSTGRES_PASSWORD=secure_password_123

# =====================================================
# BROWSER POOL CONFIGURATION
# =====================================================

# Browser Pool URLs (comma-separated)
BROWSER_POOL_URLS=http://localhost:3001,http://localhost:3002
BROWSER_POOL_1_URL=http://localhost:3001
BROWSER_POOL_2_URL=http://localhost:3002

# Browser Configuration
BROWSER_TIMEOUT=30000
BROWSER_CONCURRENT_SESSIONS=20
BROWSER_ENABLE_STEALTH=true

# =====================================================
# PERFORMANCE AND SCALING
# =====================================================

# Worker Pool Configuration
MAX_WORKERS=50
MAX_CONCURRENT_PER_WORKER=10
WORKER_TIMEOUT=300
BATCH_SIZE_DEFAULT=100

# Rate Limiting
REQUESTS_PER_MINUTE=1000
REQUESTS_PER_SECOND=20

# Memory Management
MAX_MEMORY_USAGE=8G
CLEANUP_INTERVAL_MINUTES=30

# =====================================================
# EXTERNAL SERVICES (OPTIONAL)
# =====================================================

# CAPTCHA Solving Service
CAPTCHA_API_KEY=
CAPTCHA_SERVICE=2captcha
CAPTCHA_TIMEOUT=120

# Proxy Services
PROXY_USERNAME=
PROXY_PASSWORD=
PROXY_ENDPOINTS=
PROXY_ROTATION_ENABLED=false

# Email/SMS Services for 2FA
EMAIL_SERVICE_API_KEY=
SMS_SERVICE_API_KEY=

# Cloud Storage (Optional)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_S3_BUCKET=
GCS_CREDENTIALS_PATH=
AZURE_STORAGE_CONNECTION_STRING=

# =====================================================
# SECURITY AND AUTHENTICATION
# =====================================================

# Encryption Keys (Generate with: openssl rand -base64 32)
ENCRYPTION_KEY=
SESSION_SECRET=

# Authentication
AUTH_ENABLED=false
ADMIN_USERNAME=admin
ADMIN_PASSWORD=change_me_please

# SSL/TLS Configuration
SSL_ENABLED=false
SSL_CERT_PATH=
SSL_KEY_PATH=

# =====================================================
# LOGGING AND MONITORING
# =====================================================

# Logging Configuration
LOG_LEVEL=INFO
LOG_FORMAT=json
LOG_FILE_PATH=logs/scraping.log
LOG_MAX_SIZE=100MB
LOG_BACKUP_COUNT=5

# Monitoring
PROMETHEUS_ENABLED=true
PROMETHEUS_PORT=9090
GRAFANA_ENABLED=true
GRAFANA_PORT=3000
GRAFANA_ADMIN_PASSWORD=admin123

# Alerting
SLACK_WEBHOOK_URL=
EMAIL_ALERTS_ENABLED=false
ALERT_EMAIL_TO=
ALERT_EMAIL_FROM=

# =====================================================
# DEVELOPMENT AND DEBUGGING
# =====================================================

# Environment
ENVIRONMENT=development
DEBUG=false
PYTHONPATH=/Users/stm2/Desktop/site-web/private/intelligent-crawl4ai-agent

# Testing
ENABLE_TEST_ENDPOINTS=true
MOCK_EXTERNAL_SERVICES=false

# Development Features
AUTO_RELOAD=true
PROFILING_ENABLED=false

# =====================================================
# ADVANCED CONFIGURATION
# =====================================================

# Strategy Learning
LEARNING_ENABLED=true
STRATEGY_CACHE_TTL=3600
MIN_LEARNING_SAMPLES=10

# Data Retention
DATA_RETENTION_DAYS=30
CLEANUP_OLD_JOBS=true
ARCHIVE_SUCCESSFUL_JOBS=true

# Anti-Detection
USER_AGENT_ROTATION=true
RANDOM_DELAYS=true
STEALTH_MODE=true

# Custom Headers
CUSTOM_HEADERS={"Accept-Language": "en-US,en;q=0.9"}

# =====================================================
# DOCKER CONFIGURATION
# =====================================================

# Docker Network
DOCKER_NETWORK=ai_network
DOCKER_SUBNET=172.20.0.0/16

# Container Resources
OLLAMA_MEMORY_LIMIT=8G
CHROMADB_MEMORY_LIMIT=4G
WORKERS_MEMORY_LIMIT=6G

# =====================================================
# EXAMPLE CONFIGURATIONS
# =====================================================

# Example: High-Performance Setup
# MAX_WORKERS=100
# MAX_CONCURRENT_PER_WORKER=20
# BROWSER_CONCURRENT_SESSIONS=50

# Example: Memory-Efficient Setup
# MAX_WORKERS=20
# MAX_CONCURRENT_PER_WORKER=5
# BROWSER_CONCURRENT_SESSIONS=10

# Example: Production Security
# AUTH_ENABLED=true
# SSL_ENABLED=true
# LOG_LEVEL=WARNING

# =====================================================
# NOTES
# =====================================================

# 1. Change default passwords before production use
# 2. Generate secure encryption keys
# 3. Configure external services as needed
# 4. Adjust resource limits based on your hardware
# 5. Enable monitoring for production deployments
# 6. Consider proxy rotation for large-scale scraping
# 7. Set up proper logging and alerting
